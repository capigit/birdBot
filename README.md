<div align="center">

# üê¶ BirdBot
### Assistant Ornithologique (UI + MCP + LLM)

![Python](https://img.shields.io/badge/Python-3.10%2B-blue?logo=python&logoColor=white)
![FastAPI](https://img.shields.io/badge/FastAPI-Uvicorn-009688?logo=fastapi&logoColor=white)
![PyTorch](https://img.shields.io/badge/PyTorch-ResNet50-EE4C2C?logo=pytorch&logoColor=white)
![License](https://img.shields.io/badge/License-MIT-green)

<p align="center">
  <strong>Reconnaissance d'esp√®ces ‚Ä¢ Enrichissement par IA ‚Ä¢ Interface Conversationnelle</strong>
</p>

</div>

---

Ce d√©p√¥t contient **"BirdBot"**, une application prototype compos√©e de deux services FastAPI :
1.  Une **interface web l√©g√®re (UI)** qui sert une page statique et des endpoints LLM (`chatbot_ui.py`).
2.  Un **serveur "MCP"** (Model Context Protocol) fournissant un mod√®le sp√©cialiste ResNet-50 pour l'identification d'oiseaux (`mcp_server.py`).

---

## üìù Description

* **But** : Fournir une interface conversationnelle permettant de poser des questions g√©n√©rales (via un LLM local ‚Äî Ollama) et d'identifier des oiseaux √† partir d'images en utilisant un mod√®le ResNet-50 sp√©cialis√©. Apr√®s identification, la r√©ponse est enrichie par le LLM.
* **Cas d'usage** : reconnaissance d'esp√®ces d'oiseaux (CUB-200 dataset), assistance ornithologique conversationnelle, prototype pour int√©gration LLM+vision.

## ‚ú® Fonctionnalit√©s principales

* üì∏ **Reconnaissance d'images** : endpoint `/predict` expos√© par `mcp_server.py` qui renvoie le nom de l'esp√®ce et un message de confiance.
* üß† **Enrichissement LLM** : endpoints `/llm/general` et `/llm/enrich` expos√©s par `chatbot_ui.py` qui appellent une API Ollama locale pour g√©n√©rer des r√©ponses conversationnelles.
* üñ•Ô∏è **UI web** : page statique `static/index.html` + `static/app.js` permettant d'uploader une image ou de poser une question en texte.

---

## üìÇ Architecture & Fichiers importants

* **`chatbot_ui.py`** : FastAPI app (variable `app`) qui :
    * Sert les fichiers statiques (dossier `static`) et l'UI.
    * Expose `/llm/general` pour questions g√©n√©rales.
    * Expose `/llm/enrich` pour enrichir une identification fournie par le MCP en appelant Ollama.

* **`mcp_server.py`** : FastAPI app (variable `mcp_app`) qui :
    * Charge `classes.txt` et les poids `resnet50_cub.pth`.
    * Expose `/predict` pour recevoir une image et renvoyer l'esp√®ce identifi√©e (si confiance >= 80%).

* **`static/`** :
    * `index.html`: UI principale (upload image + formulaire texte).
    * `app.js`: logique c√¥t√© client ‚Äî envoie images au MCP et questions au service LLM, g√®re l'affichage.
    * `styles.css`: styles CSS.

* **`classes.txt`** : liste des 200 classes (CUB-200) utilis√©e par le mod√®le sp√©cialiste.
* **`resnet50_cub.pth`** : poids pr√©-entra√Æn√©s (doit se trouver √† la racine) ‚Äî n√©cessaire au `mcp_server`.
* **`requirements.txt`** : d√©pendances Python requises.

> **üèãÔ∏è Entra√Ænement (retrain)**
> Un dossier `EntrainementModel` est disponible et contient les scripts et ressources permettant de r√©entra√Æner ou d'ajuster le mod√®le sp√©cialiste ResNet-50 sur vos donn√©es. Consultez ce dossier pour les instructions sp√©cifiques et les d√©pendances n√©cessaires au r√©-entrainement.

---

## üõ†Ô∏è Technologies utilis√©es

* **Langage** : Python
* **Framework web** : FastAPI + Uvicorn
* **Vision** : PyTorch (+ torchvision), PIL (Pillow)
* **Frontend** : HTML/CSS/vanilla JavaScript
* **Templating** : Jinja2 (pour servir `index.html` via FastAPI)
* **LLM** : Ollama `llama3.2:1b`

---

## ‚öôÔ∏è Pr√©requis

* Python 3.10+ (3.12 recommand√©)
* `pip` pour installer les d√©pendances
* Fichiers suivants pr√©sents √† la racine du d√©p√¥t :
    * `resnet50_cub.pth` (poids du mod√®le sp√©cialiste)
    * `classes.txt` (liste des classes)
* Service Ollama local (optionnel mais requis pour les r√©ponses LLM) avec le mod√®le `llama3.2:1b` (ou autre mod√®le compatible) disponible via `http://localhost:11434/api/generate`.
* Connexions r√©seau locales autoris√©es entre les services (CORS d√©j√† configur√© pour `mcp_server`).

---

## üöÄ Installation

1.  **Cloner le d√©p√¥t :**

```powershell
git clone https://github.com/capigit/birdBot.git
cd birdBot
```
2.  **Cr√©er un environnement virtuel et l'activer :**

<!-- end list -->

```powershell
python -m venv .venv; .\.venv\Scripts\Activate.ps1
```

3.  **Installer les d√©pendances :**

<!-- end list -->

```powershell
pip install -r requirements.txt
```

4.  **V√©rifier** que `resnet50_cub.pth` et `classes.txt` sont pr√©sents √† la racine.

5.  **(Optionnel) Installer et lancer Ollama** si vous voulez les r√©ponses LLM :

      * Reportez-vous √† la documentation d'Ollama pour l'installation et le chargement de mod√®les.

-----

## ‚ñ∂Ô∏è Ex√©cution

Vous devez lancer les deux services dans deux terminaux distincts (ou les d√©ployer s√©par√©ment).

**1. Lancer le MCP (mod√®le sp√©cialiste) :**

```powershell
uvicorn mcp_server:mcp_app --port 8001 --reload
```

**2. Lancer l'UI + endpoints LLM** (requiert Ollama pour que l'enrichissement fonctionne) :

```powershell
uvicorn chatbot_ui:app --port 8000 --reload
```

Ensuite, ouvrez `http://localhost:8000` dans votre navigateur.

-----

## üß™ Commandes utiles / Tests rapides

  * **Tester le MCP (curl) :**

<!-- end list -->

```powershell
curl -F "file=@path\to\bird.jpg" [http://127.0.0.1:8001/predict](http://127.0.0.1:8001/predict)
```

  * **Tester l'endpoint LLM g√©n√©ral (curl) :**

<!-- end list -->

```powershell
curl -X POST -H "Content-Type: application/json" -d '{"text":"Quel est le comportement hivernal du merle ?"}' [http://127.0.0.1:8000/llm/general](http://127.0.0.1:8000/llm/general)
```

  * **V√©rifier que Ollama est joignable (exemple basique) :**

<!-- end list -->

```powershell
curl -X POST http://localhost:11434/api/generate -H "Content-Type: application/json" -d "{\"model\":\"llama3.2:1b\",\"prompt\":\"Bonjour\"}"
```

-----

## üí° Exemples d'utilisation

Dans le navigateur `http://localhost:8000` :

1.  **T√©l√©versez une photo** d'un oiseau via le formulaire "Classifier Image (MCP)". Si la confiance est \>= 80% le nom d'esp√®ce sera renvoy√©, puis l'UI appellera `/llm/enrich` pour obtenir un court texte informatif.
2.  **Posez une question** en texte dans le champ pr√©vu; elle sera envoy√©e √† `/llm/general` et trait√©e par Ollama.

-----

## üîß Configuration

  * `chatbot_ui.py` contient deux constantes importantes :
      * `OLLAMA_API_URL` : URL de l'API Ollama (par d√©faut `http://localhost:11434/api/generate`).
      * `OLLAMA_MODEL` : mod√®le Ollama utilis√© (par d√©faut `llama3.2:1b`).
  * `static/app.js` contient la constante `MCP_URL` pointant vers le MCP (`http://127.0.0.1:8001/predict`).

Pour personnaliser les adresses/ports, modifiez ces constantes ou adaptez le code pour lire des variables d'environnement (ex. `os.getenv`).

-----

## ü§ù Contribution

  * **Signaler un bug** : ouvrez une issue d√©crivant le probl√®me et donn√©es de reproduction.
  * **Proposer une am√©lioration** : forkez le d√©p√¥t, ouvrez une pull request bien cibl√©e et document√©e.
  * **Tests** : aucun test automatis√© n'est fourni actuellement ‚Äî contributions pour ajouter une suite de tests sont bienvenues.

## ‚öñÔ∏è Licence

Ce projet est distribu√© sous la licence **MIT**. Voir le fichier `LICENSE` √† la racine du d√©p√¥t pour le texte complet.